{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJXW_DgiSebM"
      },
      "source": [
        "# LangGraph and LangSmith - Agentic RAG Powered by LangChain\n",
        "\n",
        "In the following notebook we'll complete the following tasks:\n",
        "\n",
        "- ü§ù Breakout Room #1:\n",
        "  1. Install required libraries\n",
        "  2. Set Environment Variables\n",
        "  3. Creating our Tool Belt\n",
        "  4. Creating Our State\n",
        "  5. Creating and Compiling A Graph!\n",
        "  \n",
        "\n",
        "- ü§ù Breakout Room #2:\n",
        "  1. Creating an Evaluation Dataset\n",
        "  2. Adding Evaluators\n",
        "  3. Evaluating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djQ3nRAgoF67"
      },
      "source": [
        "# ü§ù Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7pQDUhUnIo8"
      },
      "source": [
        "## LangGraph - Building Cyclic Applications with LangChain\n",
        "\n",
        "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
        "\n",
        "### Why Cycles?\n",
        "\n",
        "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
        "\n",
        "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effetively allowing us to recreate appliation flowcharts in code in an almost 1-to-1 fashion.\n",
        "\n",
        "### Why LangGraph?\n",
        "\n",
        "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_fLDElOVoop"
      },
      "source": [
        "## Task 1:  Dependencies\n",
        "\n",
        "We'll first install all our required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "KaVwN269EttM"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain_openai langgraph arxiv duckduckgo-search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wujPjGJuoPwg"
      },
      "source": [
        "## Task 2: Environment Variables\n",
        "\n",
        "We'll want to set both our OpenAI API key and our LangSmith environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Jdh8CoVWHRvs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dacc3a77-20e5-402b-91c6-3e8997cc974a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv0glIDyHmRt",
        "outputId": "8285e7be-463e-4afa-b936-e0837b2375b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangSmith API Key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE1 - LangGraph - {uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBRyQmEAVzua"
      },
      "source": [
        "## Task 3: Creating our Tool Belt\n",
        "\n",
        "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
        "\n",
        "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
        "\n",
        "We'll leverage:\n",
        "\n",
        "- [Duck Duck Go Web Search](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/ddg_search)\n",
        "- [Arxiv](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/arxiv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k6n_Dob2F46"
      },
      "source": [
        "####üèóÔ∏è Activity #1:\n",
        "\n",
        "Please add the tools to use into our toolbelt.\n",
        "\n",
        "> NOTE: Each tool in our toolbelt should be a method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "lAxaSvlfIeOg"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "\n",
        "tool_belt = [\n",
        "    ### YOUR TOOL HERE,\n",
        "    ### YOUR TOOL HERE,\n",
        "    DuckDuckGoSearchRun(),\n",
        "    ArxivQueryRun()\n",
        "\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FdOjEslXdRR"
      },
      "source": [
        "### Actioning with Tools\n",
        "\n",
        "Now that we've created our tool belt - we need to create a process that will let us leverage them when we need them.\n",
        "\n",
        "We'll use the built-in [`ToolExecutor`](https://github.com/langchain-ai/langgraph/blob/main/langgraph/prebuilt/tool_executor.py) to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "cFr1m80-JZsD"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolExecutor\n",
        "\n",
        "tool_executor = ToolExecutor(tool_belt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tool_executor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFACkK-PhPsY",
        "outputId": "93f748ba-f861-4ec2-9610-0c09bb0ba9d6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ToolExecutor(bound=RunnableLambda(_execute), tools=[DuckDuckGoSearchRun(), ArxivQueryRun()], tool_map={'duckduckgo_search': DuckDuckGoSearchRun(), 'arxiv': ArxivQueryRun()}, invalid_tool_msg_template='{requested_tool_name} is not a valid tool, try one of [{available_tool_names_str}].')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI-C669ZYVI5"
      },
      "source": [
        "### Model\n",
        "\n",
        "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
        "\n",
        "- OpenAI's GPT-3.5 and GPT-4\n",
        "- Anthropic's Claude\n",
        "- Google's Gemini\n",
        "\n",
        "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "QkNS8rNZJs4z"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugkj3GzuZpQv"
      },
      "source": [
        "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "4OdMqFafZ_0V"
      },
      "outputs": [],
      "source": [
        "from langchain_core.utils.function_calling import convert_to_openai_function\n",
        "\n",
        "functions = [convert_to_openai_function(t) for t in tool_belt]\n",
        "model = model.bind_functions(functions)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "functions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1gBjEOnhbhj",
        "outputId": "6f71d2de-6c53-41cf-fe2d-5c40faeeb99e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'duckduckgo_search',\n",
              "  'description': 'A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query.',\n",
              "  'parameters': {'type': 'object',\n",
              "   'properties': {'query': {'description': 'search query to look up',\n",
              "     'type': 'string'}},\n",
              "   'required': ['query']}},\n",
              " {'name': 'arxiv',\n",
              "  'description': 'A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.',\n",
              "  'parameters': {'type': 'object',\n",
              "   'properties': {'query': {'description': 'search query to look up',\n",
              "     'type': 'string'}},\n",
              "   'required': ['query']}}]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7XwvrQWh2HO",
        "outputId": "a0597a9e-bd3f-4476-dabb-15988f6308ba"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7b988c26dd50>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7b988c26cc70>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy=''), kwargs={'functions': [{'name': 'duckduckgo_search', 'description': 'A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query.', 'parameters': {'type': 'object', 'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query']}}, {'name': 'arxiv', 'description': 'A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.', 'parameters': {'type': 'object', 'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query']}}]})"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERzuGo6W18Lr"
      },
      "source": [
        "#### ‚ùì Question #1:\n",
        "\n",
        "How does the model determine which tool to use?\n",
        "\n",
        "\n",
        "### **ANSWER:\n",
        "We are bind-ing our model with the `function` after the step of  `convert_to_openai_function` which basically converts our \"tools\" into the way openai model understands and that helps the model know what tools has access to and how to call them. The way it knows is through prompt and the metadata it passes.\n",
        "\n",
        "The model has access to the tools through a prompt which it can use and it returns an output which tools to use based on the information.\n",
        "\n",
        "It looks into the keywords of the input and determines the importance and then selects/suggest the tool that best fits the descriptions of the tool."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_296Ub96Z_H8"
      },
      "source": [
        "## Putting the State in Stateful\n",
        "\n",
        "Earlier we used this phrasing:\n",
        "\n",
        "`coordinated multi-actor and stateful applications`\n",
        "\n",
        "So what does that \"stateful\" mean?\n",
        "\n",
        "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
        "\n",
        "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
        "\n",
        "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
        "\n",
        "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
        "\n",
        "1. We initialize our state object:\n",
        "  - `{\"messages\" : []}`\n",
        "2. Our user submits a query to our application.\n",
        "  - New State: `HumanMessage(#1)`\n",
        "  - `{\"messages\" : [HumanMessage(#1)}`\n",
        "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
        "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
        "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
        "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "mxL9b_NZKUdL"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated, Sequence\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[Sequence[BaseMessage], operator.add]\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWsMhfO9grLu"
      },
      "source": [
        "## It's Graphing Time!\n",
        "\n",
        "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
        "\n",
        "Let's take a second to refresh ourselves about what a graph is in this context.\n",
        "\n",
        "Graphs, also called networks in some circles, are a collection of connected objects.\n",
        "\n",
        "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
        "\n",
        "Let's look at a simple graph.\n",
        "\n",
        "![image](https://i.imgur.com/2NFLnIc.png)\n",
        "\n",
        "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
        "\n",
        "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
        "\n",
        "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
        "\n",
        "Let's create some nodes and expand on our diagram.\n",
        "\n",
        "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "91flJWtZLUrl"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolInvocation\n",
        "import json\n",
        "from langchain_core.messages import FunctionMessage\n",
        "\n",
        "def call_model(state):\n",
        "  messages = state[\"messages\"]\n",
        "  response = model.invoke(messages)\n",
        "\n",
        "  print(\"call_model - response: \", response)\n",
        "\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "\n",
        "\n",
        "def call_tool(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  print(\"call_tool - last_message: \", last_message)\n",
        "\n",
        "\n",
        "  action = ToolInvocation(\n",
        "      tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
        "      tool_input=json.loads(\n",
        "          last_message.additional_kwargs[\"function_call\"][\"arguments\"]\n",
        "      )\n",
        "  )\n",
        "\n",
        "  response = tool_executor.invoke(action)\n",
        "\n",
        "  function_message = FunctionMessage(content=str(response), name=action.tool)\n",
        "\n",
        "  return {\"messages\" : [function_message]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bwR7MgWj3Wg"
      },
      "source": [
        "Now we have two total nodes. We have:\n",
        "\n",
        "- `call_model` is a node that will...well...call the model\n",
        "- `call_tool` is a node which will call a tool\n",
        "\n",
        "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "_vF4_lgtmQNo"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"action\", call_tool)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# check the nodes\n",
        "workflow.nodes\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4aYLOOWRCt_",
        "outputId": "0ec4ddae-8f05-4b03-a171-b8afb92ab998"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'agent': RunnableLambda(call_model), 'action': RunnableLambda(call_tool)}"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8CjRlbVmRpW"
      },
      "source": [
        "Let's look at what we have so far:\n",
        "\n",
        "![image](https://i.imgur.com/md7inqG.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaXHpPeSnOWC"
      },
      "source": [
        "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "YGCbaYqRnmiw"
      },
      "outputs": [],
      "source": [
        "workflow.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUsfGoSpoF9U"
      },
      "source": [
        "![image](https://i.imgur.com/wNixpJe.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q_pQgHmoW0M"
      },
      "source": [
        "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
        "\n",
        "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
        "\n",
        "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
        "\n",
        "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
        "\n",
        "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "1BZgb81VQf9o"
      },
      "outputs": [],
      "source": [
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if \"function_call\" not in last_message.additional_kwargs:\n",
        "    return \"end\"\n",
        "\n",
        "  return \"continue\"\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cvhcf4jp0Ce"
      },
      "source": [
        "Let's visualize what this looks like.\n",
        "\n",
        "![image](https://i.imgur.com/8ZNwKI5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKCjWJCkrJb9"
      },
      "source": [
        "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "UvcgbHf1rIXZ"
      },
      "outputs": [],
      "source": [
        "workflow.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiWDwBQtrw7Z"
      },
      "source": [
        "Let's look at the final visualization.\n",
        "\n",
        "![image](https://i.imgur.com/NWO7usO.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYqDpErlsCsu"
      },
      "source": [
        "All that's left to do now is to compile our workflow - and we're off!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "zt9-KS8DpzNx"
      },
      "outputs": [],
      "source": [
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhNWIwBL1W4Q"
      },
      "source": [
        "#### ‚ùì Question #2:\n",
        "\n",
        "Is there any specific limit to how many times we can cycle?\n",
        "\n",
        "If not, how could we impose a limit to the number of cycles?\n",
        "\n",
        "\n",
        "### **ANSWER:\n",
        "No, there is no specific limit to how many times we can cycle. We can have infinite loop if we do not stop.\n",
        "\n",
        "We can impose limit in couple different ways, some listed below:\n",
        "- Add condition on the `should_continue`node  / `add_conditional_edges` and keep track of the cycles\n",
        "- We can atleast iterate on all the tools or also check the `function call` to see if all tools has been looked into... and if its done, we can exit with a condition added.\n",
        "- We can perhaps also exit using the `max iteration` parameters in the Agent. I would only want to iterate atleast to the no of tools I have and then exit.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEYcTShCsPaa"
      },
      "source": [
        "## Using Our Graph\n",
        "\n",
        "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
        "\n",
        "Let's try out a few examples to see how it fairs:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"What is RAG in the context of Large Language Models? When did it break onto the scene?\")]}\n",
        "\n",
        "app.invoke(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCN70FdViFM4",
        "outputId": "a9b27bd4-67eb-48aa-f107-c47b35eadd08"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='What is RAG in the context of Large Language Models? When did it break onto the scene?'),\n",
              "  AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"RAG in the context of Large Language Models\"}', 'name': 'duckduckgo_search'}}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 171, 'total_tokens': 196}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'function_call', 'logprobs': None}),\n",
              "  FunctionMessage(content='Retrieval Augmented Generation (RAG) is an architecture that augments the capabilities of a Large Language Model (LLM) like ChatGPT by adding an information retrieval system that provides grounding data. Adding an information retrieval system gives you control over grounding data used by an LLM when it formulates a response. Context. In a recent overview on the state of large language models (LLMs), Karpathy described LLMs as the kernel process of a new kind of operating system. Just as modern computers have RAM and access to files, LLMs have a context window that can be loaded with information retrieved from numerous data sources. RAG stands for R etrieval- A ugmented G eneration. RAG enables large language models (LLM) to access and utilize up-to-date information. Hence, it improves the quality of relevance of the response from LLM. Below is a simple diagram of how RAG is implemented. Retrieval-augmented generation (RAG) is a technique used to \"ground\" large language models (LLMs) with specific data sources, often sources that weren\\'t included in the models\\' original ... Key Takeaways. RAG is a relatively new artificial intelligence technique that can improve the quality of generative AI by allowing large language model (LLMs) to tap additional data resources without retraining. RAG models build knowledge repositories based on the organization\\'s own data, and the repositories can be continually updated to ...', name='duckduckgo_search'),\n",
              "  AIMessage(content='Retrieval Augmented Generation (RAG) is an architecture that augments the capabilities of a Large Language Model (LLM) like ChatGPT by adding an information retrieval system that provides grounding data. RAG enables LLMs to access and utilize up-to-date information, improving the quality and relevance of the responses generated by the model. RAG is a relatively new artificial intelligence technique that allows LLMs to tap into additional data resources without the need for retraining. It has been described as a technique that \"grounds\" LLMs with specific data sources, enhancing their performance. RAG broke onto the scene as a technique to improve the quality of generative AI responses by leveraging additional data sources.', response_metadata={'token_usage': {'completion_tokens': 145, 'prompt_tokens': 505, 'total_tokens': 650}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None})]}"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBHnUtLSscRr"
      },
      "source": [
        "Let's look at what happened:\n",
        "\n",
        "1. Our state object was populated with our request\n",
        "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
        "3. The conditional edge received the state object, found the \"function_call\" `additional_kwarg`, and sent the state object to the action node\n",
        "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
        "5. The agent node added a response to the state object and passed it along the conditional edge\n",
        "6. The conditional edge received the state object, could not find the \"function_call\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
        "\n",
        "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afv2BuEsV5JG",
        "outputId": "53332fae-5025-4840-dc00-d497fa046a98"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='What is QLoRA in Machine Learning? Are their any papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?'),\n",
              "  AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"QLoRA in Machine Learning\"}', 'name': 'duckduckgo_search'}}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 193, 'total_tokens': 215}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'function_call', 'logprobs': None}),\n",
              "  FunctionMessage(content=\"Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Unlock the power of QLoRA with our definitive guide! Learn how to fine-tune the Falcon-7b model using PEFT for optimal AI performance. Step into the future of machine learning today. Balancing this tradeoff is a key contribution of QLoRA. QLoRA. QLoRA (or Quantized Low-Rank Adaptation) combines 4 ingredients to get the most out of a machine's limited memory without sacrificing model performance. I will briefly summarize key points from each. More details are available in the QLoRA paper [4]. Ingredient 1: 4-bit NormalFloat Large Language Models (LLMs) are currently a hot topic in the field of machine learning. Imagine you're an ML Engineer and your company has access to GPUs and open-source LLMs like LLAMA/Falcon. ... LoRA, Machine Learning, QLoRA, transformers. Categories: data-science, machine-learning. Updated: July 26, 2023. Share on Twitter Facebook ... QLoRA is a fine-tuning method that combines Quantization and Low-Rank Adapters (LoRA). QLoRA is revolutionary in that it democratizes fine-tuning: it enables one to fine-tune massive models with billions of parameters on relatively small, highly available GPUs. ... QLoRA stands as a significant advancement in the field of machine learning ...\", name='duckduckgo_search'),\n",
              "  AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"QLoRA machine learning\"}', 'name': 'arxiv'}}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 574, 'total_tokens': 592}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'function_call', 'logprobs': None}),\n",
              "  FunctionMessage(content='Published: 2023-05-23\\nTitle: QLoRA: Efficient Finetuning of Quantized LLMs\\nAuthors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\\nSummary: We present QLoRA, an efficient finetuning approach that reduces memory usage\\nenough to finetune a 65B parameter model on a single 48GB GPU while preserving\\nfull 16-bit finetuning task performance. QLoRA backpropagates gradients through\\na frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\\nprevious openly released models on the Vicuna benchmark, reaching 99.3% of the\\nperformance level of ChatGPT while only requiring 24 hours of finetuning on a\\nsingle GPU. QLoRA introduces a number of innovations to save memory without\\nsacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\\ninformation theoretically optimal for normally distributed weights (b) double\\nquantization to reduce the average memory footprint by quantizing the\\nquantization constants, and (c) paged optimziers to manage memory spikes. We\\nuse QLoRA to finetune more than 1,000 models, providing a detailed analysis of\\ninstruction following and chatbot performance across 8 instruction datasets,\\nmultiple model types (LLaMA, T5), and model scales that would be infeasible to\\nrun with regular finetuning (e.g. 33B and 65B parameter models). Our results\\nshow that QLoRA finetuning on a small high-quality dataset leads to\\nstate-of-the-art results, even when using smaller models than the previous\\nSoTA. We provide a detailed analysis of chatbot performance based on both human\\nand GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\\nalternative to human evaluation. Furthermore, we find that current chatbot\\nbenchmarks are not trustworthy to accurately evaluate the performance levels of\\nchatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\\nChatGPT. We release all of our models and code, including CUDA kernels for\\n4-bit training.\\n\\nPublished: 2024-02-08\\nTitle: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nAuthors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\\nSummary: The LoRA-finetuning quantization of LLMs has been extensively studied to\\nobtain accurate yet compact LLMs for deployment on resource-constrained\\nhardware. However, existing methods cause the quantized LLM to severely degrade\\nand even fail to benefit from the finetuning of LoRA. This paper proposes a\\nnovel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\\nthrough information retention. The proposed IR-QLoRA mainly relies on two\\ntechnologies derived from the perspective of unified information: (1)\\nstatistics-based Information Calibration Quantization allows the quantized\\nparameters of LLM to retain original information accurately; (2)\\nfinetuning-based Information Elastic Connection makes LoRA utilizes elastic\\nrepresentation transformation with diverse information. Comprehensive\\nexperiments show that IR-QLoRA can significantly improve accuracy across LLaMA\\nand LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%\\nimprovement on MMLU compared with the state-of-the-art methods. The significant\\nperformance gain requires only a tiny 0.31% additional time consumption,\\nrevealing the satisfactory efficiency of our IRQLoRA. We highlight that\\nIR-QLoRA enjoys excellent versatility, compatible with various frameworks\\n(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.\\nThe code is available at https://github.com/htqin/ir-qlora.\\n\\nPublished: 2024-02-16\\nTitle: QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large Language Model Tuning\\nAuthors: Hossein Rajabzadeh, Mojtaba Valipour, Tianshu Zhu, Marzieh Tahaei, Hyock Ju Kwon, Ali Ghodsi, Boxing Chen, Mehdi Rezagholizadeh\\nSummary: Finetuning large language models requires huge GPU memory, restricting the\\nchoice to acq', name='arxiv'),\n",
              "  AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"Tim Dettmers bio\"}', 'name': 'duckduckgo_search'}}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 1584, 'total_tokens': 1605}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'function_call', 'logprobs': None}),\n",
              "  FunctionMessage(content=\"Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ...\", name='duckduckgo_search'),\n",
              "  AIMessage(content='QLoRA (Quantized Low-Rank Adaptation) is an efficient fine-tuning approach in machine learning that reduces memory usage while preserving performance. It combines quantization and low-rank adapters to enable fine-tuning of massive models on relatively small GPUs.\\n\\nOne of the papers related to QLoRA is titled \"QLoRA: Efficient Finetuning of Quantized LLMs\" by Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. The paper presents QLoRA as an approach to finetune large language models efficiently, achieving state-of-the-art results with reduced memory usage.\\n\\nTim Dettmers is one of the authors of the QLoRA paper. His research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. He works on developing compression and networking algorithms to enable memory-efficient, fast, and cost-effective deep learning solutions.', response_metadata={'token_usage': {'completion_tokens': 196, 'prompt_tokens': 1895, 'total_tokens': 2091}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None})]}"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"What is QLoRA in Machine Learning? Are their any papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?\")]}\n",
        "\n",
        "app.invoke(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXzDlZVz1Hnf"
      },
      "source": [
        "####üèóÔ∏è Activity #2:\n",
        "\n",
        "Please write out the steps the agent took to arrive at the correct answer.\n",
        "\n",
        "###**ANSWER:\n",
        "\n",
        "- The app begins with the initial user question triggering the passing of a state object to the agent node.\n",
        "-The LLM decides to invoke the duckduckgo tool, adding an AIMessage to the state.\n",
        "-Analysis by the conditional edge identifies a \"function_call\" in the last message, leading to movement to the action node.\n",
        "-In the action node, the duckduckgo tool is invoked, and its response is added to the state as a FunctionMessage.\n",
        "-The agent then decides to invoke the arxiv tool, which is included as an AIMessage in the state.\n",
        "-The conditional node directs the state to the action node once more, where the arxiv tool is executed, and its result is recorded as a FunctionMessage.\n",
        "-Another invocation of the duckduckgo tool occurs within the agent node to search for Tim Dettmer's biography, following similar steps as before.\n",
        "-Upon revisiting the agent node, a decision is made that there is enough information to respond to the initial query, adding an AIMessage without the \"function_call\" tag to signal the process's completion at the end node.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV3XeFOT1Sar"
      },
      "source": [
        "### Pre-processing for LangSmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wruQCuzewUuO"
      },
      "source": [
        "To do a little bit more preprocessing, let's wrap our LangGraph agent in a simple chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "oeXdQgbxwhTv"
      },
      "outputs": [],
      "source": [
        "def convert_inputs(input_object):\n",
        "  return {\"messages\" : [HumanMessage(content=input_object[\"question\"])]}\n",
        "\n",
        "def parse_output(input_state):\n",
        "  return input_state[\"messages\"][-1].content\n",
        "\n",
        "agent_chain = convert_inputs | app | parse_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "orYxBZXSxJjZ",
        "outputId": "8d9f6ddc-76de-4740-fb08-a8a10e6728fe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"RAG stands for Retrieval-augmented generation. It is an AI framework that improves the quality of language model-generated responses by grounding the model on external sources of knowledge to supplement the model's internal representation of information. RAG ensures that the model has access to current and reliable facts from external sources, enhancing the accuracy and reliability of generative AI models.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "agent_chain.invoke({\"question\" : \"What is RAG?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQmrzYfrm1Dr"
      },
      "source": [
        "# ü§ù Breakout Room #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9UkCIqkpyZu"
      },
      "source": [
        "## Task 1: Creating An Evaluation Dataset\n",
        "\n",
        "Just as we saw last week, we'll want to create a dataset to test our Agent's ability to answer questions.\n",
        "\n",
        "In order to do this - we'll want to provide some questions and some answers. Let's look at how we can create such a dataset below.\n",
        "\n",
        "```python\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfMXF2KAsQxs"
      },
      "source": [
        "####üèóÔ∏è Activity #3:\n",
        "\n",
        "Please create a dataset in the above format with at least 5 questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "CbagRuJop83E"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    ### YOUR QUESTIONS HERE\n",
        "\n",
        "    'What innovative technique does QLoRA introduce to allow efficient fine-tuning of quantized large language models on limited hardware resources?',\n",
        "    'How does QLoRA maintain the performance of a 65B parameter model while significantly reducing its memory requirements during fine-tuning?',\n",
        "    'In what way does QLoRA outperform existing models on the Vicuna benchmark, and what is the significance of this achievement?',\n",
        "    'What novel data type introduced by QLoRA is deemed information-theoretically optimal for normally distributed weights in language models?',\n",
        "    'How does QLoRAs approach to memory management during model training differ from traditional methods, enhancing its efficiency?',\n",
        "    'Based on QLoRAs findings, what implications does the quality of data have on the performance of fine-tuned chatbot models compared to the size of the dataset?',\n",
        "\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    ### YOUR ANSWERS HERE\n",
        "\n",
        "    {\"must_mention\" : ['4-bit NormalFloat', 'Double Quantization']},\n",
        "    {\"must_mention\" : ['Low Rank Adapters (LoRA)', 'backpropagating gradients']},\n",
        "    {\"must_mention\" : ['Guanaco model family', '99.3%']},\n",
        "    {\"must_mention\" : ['NF4', 'Quantization']},\n",
        "    {\"must_mention\" : ['Paged Optimizers', 'Unified Memory']},\n",
        "    {\"must_mention\" : ['Data Quality Importance', 'Dataset Size']},\n",
        "]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7QVFuAmsh7L"
      },
      "source": [
        "Now we can add our dataset to our LangSmith project using the following code which we saw last Thursday!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "RLfrZrgSsn85"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "dataset_name = f\"Retrieval Augmented Generation - Evaluation Dataset - {uuid4().hex[0:8]}\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    outputs=answers,\n",
        "    dataset_id=dataset.id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciV73F9Q04w0"
      },
      "source": [
        "#### ‚ùì Question #3:\n",
        "\n",
        "How are the correct answers associated with the questions?\n",
        "\n",
        "> NOTE: Feel free to indicate if this is problematic or not\n",
        "\n",
        "\n",
        "###**ANSWER:\n",
        "\n",
        "- The correct answers are associated with question because we wrote a relationship to connect questions and answers where we added a contigency that the answer \"must contain\" keywords\n",
        "- Programitically it is associated with this func `client.create_examples(...)`\n",
        "\n",
        "\n",
        "\n",
        "- This could be problematic in varioous ways. Such as:\n",
        "-- In QA consistent ordering!\n",
        "-- User may correctly answer many questions without explicitly using the keywords on \"must contain\"\n",
        "-- In somecases, the context could get outdated but if the pairs are still used without update then it could be problematic\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lRTXUrTtP9Y"
      },
      "source": [
        "## Task 2: Adding Evaluators\n",
        "\n",
        "Now we can add a custom evaluator to see if our responses contain the expected information.\n",
        "\n",
        "We'll be using a fairly naive exact-match process to determine if our response contains specific strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "QrAUXMFftlAY"
      },
      "outputs": [],
      "source": [
        "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
        "\n",
        "@run_evaluator\n",
        "def must_mention(run, example) -> EvaluationResult:\n",
        "    prediction = run.outputs.get(\"output\") or \"\"\n",
        "    required = example.outputs.get(\"must_mention\") or []\n",
        "    score = all(phrase in prediction for phrase in required)\n",
        "    return EvaluationResult(key=\"must_mention\", score=score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNtHORUh0jZY"
      },
      "source": [
        "#### ‚ùì Question #4:\n",
        "\n",
        "What are some ways you could improve this metric as-is?\n",
        "\n",
        "> NOTE: Alternatively you can suggest where gaps exist in this method.\n",
        "\n",
        "\n",
        "###**ANSWER:\n",
        "\n",
        "- As per langsmith metrics... we can see that if its not a exact match, its not that happy about it. Somethimes this could lead to false decision making if we do not look into the data properly.\n",
        "\n",
        "- Few room for improvements could be:\n",
        "-- use all options available (possibly generate using LLMs)\n",
        "-- implement case-insensitive methods\n",
        "-- use classic nlp methods such as synonyms, lexical equivalent methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ4DVSXl0BX5"
      },
      "source": [
        "Now that we have created our custom evaluator - let's initialize our `RunEvalConfig` with it, and a few others:\n",
        "\n",
        "- `\"criteria\"` includes the default criteria which, in this case, means \"helpfulness\"\n",
        "- `\"cot_qa\"` includes a criteria that bases whether or not the answer is correct by utilizing a Chain of Thought prompt and the provided context to determine if the response is correct or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "sL4-XcjytWsu"
      },
      "outputs": [],
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[must_mention],\n",
        "    evaluators=[\n",
        "        \"criteria\",\n",
        "        \"cot_qa\",\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1RJr349zhv7"
      },
      "source": [
        "Task 3: Evaluating\n",
        "\n",
        "All that is left to do is evaluate our agent's response!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5TeCUUkuGld",
        "outputId": "7dbfb68c-c1cc-44f0-c8ae-2bb125d50bfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for project 'RAG Pipeline - Evaluation - e031c2d0' at:\n",
            "https://smith.langchain.com/o/8d921a96-95b4-5fe0-8fe1-a093c1191673/datasets/b0aef882-f3f9-4979-a5e6-5f583e9c8f2a/compare?selectedSessions=173da1ba-0506-443f-8fff-e2d2ead85f2b\n",
            "\n",
            "View all tests for Dataset Retrieval Augmented Generation - Evaluation Dataset - c5410119 at:\n",
            "https://smith.langchain.com/o/8d921a96-95b4-5fe0-8fe1-a093c1191673/datasets/b0aef882-f3f9-4979-a5e6-5f583e9c8f2a\n",
            "[------------------------------------------------->] 6/6\n",
            " Experiment Results:\n",
            "        feedback.helpfulness  feedback.COT Contextual Accuracy feedback.must_mention error  execution_time                                run_id\n",
            "count                   6.00                              6.00                     6     0            6.00                                     6\n",
            "unique                   NaN                               NaN                     2     0             NaN                                     6\n",
            "top                      NaN                               NaN                 False   NaN             NaN  69d17795-6cf6-418c-bf2a-d8f2c2a5cb47\n",
            "freq                     NaN                               NaN                     4   NaN             NaN                                     1\n",
            "mean                    1.00                              0.83                   NaN   NaN            5.27                                   NaN\n",
            "std                     0.00                              0.41                   NaN   NaN            0.95                                   NaN\n",
            "min                     1.00                              0.00                   NaN   NaN            4.16                                   NaN\n",
            "25%                     1.00                              1.00                   NaN   NaN            4.65                                   NaN\n",
            "50%                     1.00                              1.00                   NaN   NaN            5.01                                   NaN\n",
            "75%                     1.00                              1.00                   NaN   NaN            6.08                                   NaN\n",
            "max                     1.00                              1.00                   NaN   NaN            6.48                                   NaN\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'project_name': 'RAG Pipeline - Evaluation - e031c2d0',\n",
              " 'results': {'8e9c0166-7a28-465f-9d88-b52c2b01055e': {'input': {'question': 'What innovative technique does QLoRA introduce to allow efficient fine-tuning of quantized large language models on limited hardware resources?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\n1. The submission provides a clear and concise explanation of the innovative technique introduced by QLoRA. It explains that QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA). This is helpful as it directly answers the question asked in the input.\\n\\n2. The submission further explains the benefits of this technique, stating that it allows for efficient fine-tuning of quantized large language models on limited hardware resources. This is insightful as it provides additional information about why this technique is innovative and useful.\\n\\n3. The submission is appropriate as it directly addresses the question asked in the input and provides a detailed and accurate response.\\n\\nBased on this analysis, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('f58bdbb6-2f86-4a47-a0d9-bb9751e34f1f'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer correctly identifies that QLoRA introduces the technique of backpropagating gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA). This is in line with the context provided, which mentions '4-bit NormalFloat', a term related to the quantization process. The student also correctly explains that this technique allows for efficient fine-tuning of quantized large language models on limited hardware resources. The additional information about reducing memory usage and preserving full 16-bit fine-tuning task performance does not conflict with any information in the context. Therefore, the student's answer is factually accurate.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('d6e53987-6817-4138-8ea9-b7c255139225'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 4.653114,\n",
              "   'run_id': '69d17795-6cf6-418c-bf2a-d8f2c2a5cb47',\n",
              "   'output': 'QLoRA introduces the innovative technique of backpropagating gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA). This technique allows for efficient fine-tuning of quantized large language models on limited hardware resources, reducing memory usage enough to fine-tune a 65B parameter model on a single 48GB GPU while preserving full 16-bit fine-tuning task performance.',\n",
              "   'reference': {'must_mention': ['4-bit NormalFloat',\n",
              "     'Double Quantization']}},\n",
              "  'e5ec21e3-dfa0-41b0-bf34-39f95fee7430': {'input': {'question': 'How does QLoRA maintain the performance of a 65B parameter model while significantly reducing its memory requirements during fine-tuning?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\n1. The submission provides a detailed explanation of how QLoRA maintains the performance of a 65B parameter model while significantly reducing its memory requirements during fine-tuning. It explains the process of backpropagating gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA). \\n\\n2. The submission also provides specific data on the memory requirements for fine-tuning the model, stating that they are reduced from over 780GB to less than 48GB. \\n\\n3. The submission concludes by stating that this reduction in memory requirements does not degrade the runtime or predictive performance compared to a 16-bit fully fine-tuned model. \\n\\nBased on these points, the submission is helpful as it provides a clear and detailed explanation of the process, and it is insightful as it provides specific data on the memory requirements. It is also appropriate as it directly answers the question asked in the input. \\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('b3ec4da6-8fd1-4b95-82e4-b7ad60b62929'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer correctly identifies that QLoRA reduces memory usage significantly, allowing the fine-tuning of a 65B parameter model on a single 48GB GPU while maintaining the performance of the model. The student also correctly explains that this is achieved by backpropagating gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA). The student further provides additional information about the memory requirements for fine-tuning the model being reduced from over 780GB to less than 48GB, without degrading the runtime or predictive performance compared to a 16-bit fully fine-tuned model. This additional information does not conflict with the context provided. Therefore, the student's answer is factually accurate.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('7d1216db-171e-4f55-9251-8e1340ac95ca'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 4.652581,\n",
              "   'run_id': 'f88b889c-39c8-4868-90e9-39de2a5e58d0',\n",
              "   'output': 'QLoRA is an efficient fine-tuning approach that reduces memory usage significantly, allowing the fine-tuning of a 65B parameter model on a single 48GB GPU while maintaining the performance of the model. This is achieved by backpropagating gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA). With QLoRA, the memory requirements for fine-tuning the model are reduced from over 780GB to less than 48GB, without degrading the runtime or predictive performance compared to a 16-bit fully fine-tuned model.',\n",
              "   'reference': {'must_mention': ['Low Rank Adapters (LoRA)',\n",
              "     'backpropagating gradients']}},\n",
              "  '743774d0-0c82-49bc-978f-9e8e436baefb': {'input': {'question': 'In what way does QLoRA outperform existing models on the Vicuna benchmark, and what is the significance of this achievement?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a detailed explanation of how QLoRA, specifically the Guanaco model family, outperforms all previous models on the Vicuna benchmark. It gives a specific performance level, which is 99.3% of the performance level of ChatGPT. This information is helpful for understanding the extent of the model\\'s superiority over previous models.\\n\\nThe submission also explains the significance of this achievement. It states that this achievement demonstrates the effectiveness of the QLoRA method in fine-tuning large language models with limited computational resources. This information is insightful as it provides context on why this achievement is important.\\n\\nThe submission further elaborates on the efficiency and effectiveness of the approach, stating that the Guanaco model was able to achieve this performance level with just 24 hours of finetuning on a 65 billion parameter model. This information is helpful for understanding the efficiency of the model.\\n\\nLastly, the submission discusses the implications of this advancement in model performance for natural language processing tasks and the potential for further advancements in the field. This information is appropriate and insightful as it provides a broader perspective on the impact of this achievement.\\n\\nTherefore, the submission is helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('5ee16ab3-4371-44e7-8623-5393d84786b4'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=0, value='INCORRECT', comment=\"The student's answer is incorrect. The context provided does not mention QLoRA, the Vicuna benchmark, or the significance of the achievement. The context only mentions the Guanaco model family and a percentage of 99.3%. The student's answer does not align with the provided context.\\nGRADE: INCORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('2acc41e2-03e5-4a31-bca8-3619144feabd'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 6.312399,\n",
              "   'run_id': 'f32d15ce-2b04-4298-86e9-f0c1360c214b',\n",
              "   'output': 'QLoRA, specifically the Guanaco model family, outperforms all previous openly released models on the Vicuna benchmark by reaching 99.3% of the performance level of ChatGPT. This achievement is significant because it demonstrates the effectiveness of the QLoRA method in fine-tuning large language models with limited computational resources. The Guanaco model was able to achieve this performance level with just 24 hours of finetuning on a 65 billion parameter model, showcasing the efficiency and effectiveness of the approach. This advancement in model performance has implications for natural language processing tasks and showcases the potential for further advancements in the field.',\n",
              "   'reference': {'must_mention': ['Guanaco model family', '99.3%']}},\n",
              "  '29880bda-9139-498b-a0ae-b95c3be8d14f': {'input': {'question': 'What novel data type introduced by QLoRA is deemed information-theoretically optimal for normally distributed weights in language models?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a direct answer to the question asked in the input. It identifies the novel data type introduced by QLoRA that is deemed information-theoretically optimal for normally distributed weights in language models, which is the 4-bit NormalFloat (NF4) data type. \\n\\nThe submission is helpful because it provides the exact information asked for in the input. It is insightful as it provides a specific detail about the data type introduced by QLoRA. The submission is also appropriate as it directly addresses the question and does not include any irrelevant or inappropriate information. \\n\\nTherefore, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('e1eaab5e-a320-4509-8da2-16fb0c270239'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The context mentions 'NF4' and 'Quantization', which are related to data types and language models. The student's answer mentions the '4-bit NormalFloat (NF4)' data type, which matches with the 'NF4' mentioned in the context. The student's answer also correctly states that this data type is deemed information-theoretically optimal for normally distributed weights in language models, which is the specific detail asked in the question. Therefore, the student's answer is factually accurate.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('b4652f57-cad6-4e08-9991-38b857cf3f2d'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 4.15978,\n",
              "   'run_id': 'ac808540-2e99-4291-82df-e55851c8dd52',\n",
              "   'output': 'The novel data type introduced by QLoRA that is deemed information-theoretically optimal for normally distributed weights in language models is the 4-bit NormalFloat (NF4) data type.',\n",
              "   'reference': {'must_mention': ['NF4', 'Quantization']}},\n",
              "  '40972de1-5805-4b76-a0ad-174b1c2cacf9': {'input': {'question': 'How does QLoRAs approach to memory management during model training differ from traditional methods, enhancing its efficiency?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\n1. The submission provides a detailed explanation of how QLoRA\\'s approach to memory management during model training differs from traditional methods. It explains the use of paged optimizers to mitigate memory spikes during gradient checkpointing, which is a specific and insightful detail.\\n\\n2. The submission further explains how this approach optimizes memory management by offloading optimizer parameters to the CPU when needed. This is a helpful detail that adds to the understanding of the process.\\n\\n3. The submission also explains the benefits of this approach, stating that it reduces memory usage, compresses the model into a smaller form factor, allows for more efficient training on consumer hardware, and enables finetuning of large models on GPUs with limited memory capacity. These details are appropriate and provide a clear understanding of the advantages of QLoRA\\'s approach.\\n\\nBased on these points, the submission is helpful, insightful, and appropriate. Therefore, it meets the criterion.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('16ef0af7-dbec-415b-9eb3-d10e07d68c82'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer correctly identifies that QLoRA's approach to memory management during model training differs from traditional methods by using paged optimizers. The student also correctly explains that this approach mitigates memory spikes during gradient checkpointing, offloads optimizer parameters to the CPU when needed, reduces memory usage, compresses the model into a smaller form factor, and allows for more efficient training on consumer hardware and finetuning of large models on GPUs with limited memory capacity. The student's answer does not conflict with the context provided.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('061e2a65-a290-431b-8a05-e618f46ae73c'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 6.47995,\n",
              "   'run_id': 'f43a9131-4037-4b8a-be82-96ef7ca4f5a3',\n",
              "   'output': \"QLoRA's approach to memory management during model training differs from traditional methods by utilizing paged optimizers to mitigate memory spikes during gradient checkpointing. This approach optimizes memory management during training by offloading optimizer parameters to the CPU when needed, reducing memory usage and compressing the model into a smaller form factor. This enhancement allows for more efficient training on consumer hardware and enables finetuning of large models on GPUs with limited memory capacity.\",\n",
              "   'reference': {'must_mention': ['Paged Optimizers', 'Unified Memory']}},\n",
              "  'e6ca5783-7206-4b1f-937f-9db49248845f': {'input': {'question': 'Based on QLoRAs findings, what implications does the quality of data have on the performance of fine-tuned chatbot models compared to the size of the dataset?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment=\"The criterion for this task is the helpfulness of the submission. \\n\\nThe submission provides a detailed explanation of QLoRA's findings and their implications on the performance of fine-tuned chatbot models. It explains how the quality of data can lead to state-of-the-art results even with smaller datasets and smaller models. \\n\\nThe submission also provides additional information about QLoRA's findings, such as the performance of 4-bit QLoRA with NF4 data type compared to 16-bit full fine-tuning and 16-bit LoRA fine-tuning. This information is relevant and provides further insight into the importance of data quality in fine-tuning chatbot models.\\n\\nThe submission concludes by reiterating the importance of data quality in achieving state-of-the-art performance in fine-tuned chatbot models. This conclusion is appropriate and reinforces the main point of the submission.\\n\\nBased on this analysis, the submission is helpful, insightful, and appropriate. Therefore, it meets the criterion.\\n\\nY\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('7ccf7952-0cca-479c-9813-8aeed98923f0'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer is in line with the context provided. The student correctly identifies that QLoRA's findings suggest the quality of data has significant implications on the performance of fine-tuned chatbot models compared to the size of the dataset. The student also correctly explains that fine-tuning on a small high-quality dataset can lead to state-of-the-art results, even when using smaller models. The student further elaborates on the findings of QLoRA, providing additional information that supports the main point. Therefore, the student's answer is factually accurate.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('679aba41-3214-4214-bfdf-5118b5fafd9e'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 5.362809,\n",
              "   'run_id': '208290f4-7823-4eb8-9589-77c1955857bf',\n",
              "   'output': \"Based on QLoRA's findings, the quality of data has significant implications on the performance of fine-tuned chatbot models compared to the size of the dataset. QLoRA's research shows that fine-tuning on a small high-quality dataset can lead to state-of-the-art results, even when using smaller models than the previous state-of-the-art models. This optimized method allows for fine-tuning of large language models using just a single GPU while maintaining high performance.\\n\\nAdditionally, QLoRA's findings suggest that 4-bit QLoRA with NF4 data type matches the performance of 16-bit full fine-tuning and 16-bit LoRA fine-tuning on academic benchmarks with well-established evaluation setups. This indicates that data quality plays a crucial role in the performance of fine-tuned chatbot models, and using high-quality data can lead to improved results even with smaller datasets.\\n\\nOverall, the quality of data used for fine-tuning chatbot models is essential for achieving state-of-the-art performance, and it can have a significant impact on the model's performance compared to the size of the dataset.\",\n",
              "   'reference': {'must_mention': ['Data Quality Importance',\n",
              "     'Dataset Size']}}},\n",
              " 'aggregate_metrics': None}"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=agent_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    project_name=f\"RAG Pipeline - Evaluation - {uuid4().hex[0:8]}\",\n",
        "    project_metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n2G714_uWf6G"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}